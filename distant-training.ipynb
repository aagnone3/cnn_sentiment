{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tchen/homebrew/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named num2words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a84992c87ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnum2words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named num2words"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "from keras.models import Model\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import num2words\n",
    "import gensim\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker\n",
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numDimensions = 300\n",
    "maxSeqLength = 250\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"don’t\": \"do not\",\n",
    "\"i'm\": \"i am\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'l\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"\\'em\", \" them\", phrase)\n",
    "    phrase = re.sub(r\"\\'nt\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'\", \" \", phrase)\n",
    "    \n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’l\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"\\’em\", \" them\", phrase)\n",
    "    phrase = re.sub(r\"\\’nt\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’\", \" \", phrase)\n",
    "    \n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySpellChecker():\n",
    "\n",
    "    def __init__(self, dict_name='en_US', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "\n",
    "        if suggestions:\n",
    "            for suggestion in suggestions:\n",
    "                if edit_distance(word, suggestion) <= self.max_dist:\n",
    "                    return suggestions[0]\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "\n",
    "    with open('emoji/emoji_ranks.json', 'r') as fn:\n",
    "        e_data = json.load(fn)\n",
    "    \n",
    "    pos_emoticons = e_data['pos']\n",
    "    neg_emoticons = e_data['neg']\n",
    "    neutral_emoticons = e_data['neu']\n",
    "        \n",
    "    # Emails\n",
    "    emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "    # Mentions\n",
    "    userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "\n",
    "    #Urls\n",
    "    urlsRegex=re.compile(r'(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "    #Numerics\n",
    "    numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "    punctuationNotEmoticonsRegex=re.compile(r'([!?.,-]){2,}')\n",
    "    \n",
    "    elongatedWords = re.compile(r'\\b(\\S*?)(.)\\2{2,}\\b')\n",
    "    allCaps = re.compile(r\"((?![<]*}) [A-Z][A-Z]+)\")\n",
    "\n",
    "    emoticonsDict = {}\n",
    "    for i,each in enumerate(pos_emoticons):\n",
    "        emoticonsDict[each]= ' <SMILE> '\n",
    "    for i,each in enumerate(neg_emoticons):\n",
    "        emoticonsDict[each]=' <SADFACE> '\n",
    "    for i,each in enumerate(neutral_emoticons):\n",
    "        emoticonsDict[each]=' <NEUTRALFACE> '\n",
    "    # use these three lines to do the replacement\n",
    "    rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "    emoticonsPattern = re.compile(\"|\".join(rep.keys()))\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    u1 = []\n",
    "    u2 = []\n",
    "    u3 = []\n",
    "    indices = []\n",
    "#     my_spell_checker = MySpellChecker(max_dist=1)\n",
    "    df = pd.read_csv('Sentiment-Analysis-Dataset.csv', sep=',')\n",
    "    sentences = df.SentimentText.values\n",
    "    sentiment = df.Sentiment.values\n",
    "    \n",
    "    for label, line in zip(sentiment,sentences):\n",
    "        # Convert multiple instances of . ? ! , to single instance\n",
    "        # okay...sure -> okay . sure\n",
    "        # okay???sure -> okay ? sure\n",
    "        # Add whitespace around such punctuation\n",
    "        # okay!sure -> okay ! sure\n",
    "#             repeatedChars = ['.', '?', '!', ',']\n",
    "\n",
    "        line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.strip())\n",
    "        line = userMentionsRegex.sub(' <USER> ', line )\n",
    "        line = emailsRegex.sub(' <EMAIL> ', line )\n",
    "        line = urlsRegex.sub(' <URL> ', line)\n",
    "        line = numsRegex.sub(' <NUMBER> ',line)\n",
    "        line = punctuationNotEmoticonsRegex.sub(r' \\1 <REPEAT> ',line)\n",
    "        line = elongatedWords.sub(r'\\1\\2 <ELONG> ', line)\n",
    "        line = allCaps.sub(r'\\1 <ALLCAPS> ', line)\n",
    "        line = re.sub('([.,!?])', r' \\1 ', line)\n",
    "        line = re.sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", r\" <NUMBER> \", line)\n",
    "        line = re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "#             chkr = SpellChecker(\"en_US\", line)\n",
    "#             for err in chkr:\n",
    "#                 err.replace(my_spell_checker.replace(err.word))\n",
    "#             line = chkr.get_text()\n",
    "\n",
    "        if mode == \"train\":\n",
    "            # Train data contains id, 3 turns and label\n",
    "            labels.append(label)\n",
    "\n",
    "        # Remove any duplicate spaces\n",
    "        duplicateSpacePattern = re.compile(r'\\ +')\n",
    "        conv = re.sub(duplicateSpacePattern, ' ', line)\n",
    "        conversations.append(line.lower())\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        return  labels, conversations\n",
    "    else:\n",
    "        return indices, conversations, u1, u2, u3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex, out_of_vocab):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    \n",
    "    # Load the embedding vectors from ther GloVe file #glove.twitter.27B.100d.txt #glove.840B.300d.txt #glove.twitter.emoji.100d.txt\n",
    "    #with io.open(os.path.join(gloveDir, 'glove.twitter.emoji.100d.txt'), encoding=\"utf8\") as f:\n",
    "    vocab = []\n",
    "    with io.open(gloveDir, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "           # print(values)\n",
    "            word = values[0]\n",
    "            vocab.append(word)\n",
    "            embeddingVector = np.array([float(val) for val in values[1:]])\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "    \n",
    "    #model = gensim.models.KeyedVectors.load_word2vec_format(gloveDir, binary=False)\n",
    "    #vocab = model.vocab.keys()\n",
    "\n",
    "    # Minimum word index of any word is 1. \n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if word not in vocab:\n",
    "            out_of_vocab.append(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "    \n",
    "    return embeddingMatrix, embeddingsIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'testBaseline.config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-62746ba55cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# args = parser.parse_args()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testBaseline.config'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfigfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'testBaseline.config'"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Baseline Script for SemEval\")\n",
    "# parser.add_argument('-config', help='Config to read details', required=True)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "with open('testBaseline.config') as configfile:\n",
    "    config = json.load(configfile)\n",
    "\n",
    "global trainDataPath, testDataPath, solutionPath, gloveDir\n",
    "global NUM_FOLDS, NUM_CLASSES, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM\n",
    "global BATCH_SIZE, LSTM_DIM, DROPOUT, NUM_EPOCHS, LEARNING_RATE    \n",
    "\n",
    "trainDataPath = config[\"train_data_path\"]\n",
    "valDataPath = config[\"val_data_path\"]\n",
    "testDataPath = config[\"test_data_path\"]\n",
    "solutionPath = config[\"solution_path\"]\n",
    "gloveDir = config[\"glove_dir\"]\n",
    "\n",
    "NUM_FOLDS = config[\"num_folds\"]\n",
    "NUM_CLASSES = config[\"num_classes\"]\n",
    "MAX_NB_WORDS = config[\"max_nb_words\"]\n",
    "MAX_SEQUENCE_LENGTH = config[\"max_sequence_length\"]\n",
    "EMBEDDING_DIM = config[\"embedding_dim\"]\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "LSTM_DIM = config[\"lstm_dim\"]\n",
    "DROPOUT = config[\"dropout\"]\n",
    "LEARNING_RATE = config[\"learning_rate\"]\n",
    "NUM_EPOCHS = config[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainDataPath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-16529e28df01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainTexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDataPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# writeNormalisedData(trainDataPath, trainTexts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"Processing val data...\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainDataPath' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "labels, trainTexts = preprocessData(trainDataPath, mode=\"train\")\n",
    "# Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable   \n",
    "# writeNormalisedData(trainDataPath, trainTexts)\n",
    "# print(\"Processing val data...\")\n",
    "# valIndices, valTexts, vallabels, u1_val, u2_val, u3_val = preprocessData(valDataPath, mode=\"train\")\n",
    "# print(\"Processing test data...\")\n",
    "# testIndices, testTexts, u1_test, u2_test, u3_test = preprocessData(testDataPath, mode=\"test\")\n",
    "# writeNormalisedData(testDataPath, testTexts)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=False)\n",
    "print(\"Extracting tokens...\")\n",
    "vocab = []\n",
    "for sent in u1_train+u2_train+u3_train:\n",
    "    vocab.extend(tokenizer.tokenize(sent))\n",
    "    \n",
    "wordIndex = {} \n",
    "for i, word in enumerate(list(set(vocab))):\n",
    "    wordIndex[word] = i+1\n",
    "\n",
    "# u1_trainToken, u2_trainToken, u3_trainToken = [tokenizer.tokenize(x) for x in u1_train], [tokenizer.tokenize(x) for x in u2_train], [tokenizer.tokenize(x) for x in u3_train]\n",
    "# u1_valToken, u2_valToken, u3_valToken = [tokenizer.tokenize(x) for x in u1_val], [tokenizer.tokenize(x) for x in u2_val], [tokenizer.tokenize(x) for x in u3_val]\n",
    "# u1_testToken, u2_testToken, u3_testToken = [tokenizer.tokenize(x) for x in u1_test], [tokenizer.tokenize(x) for x in u2_test], [tokenizer.tokenize(x) for x in u3_test]\n",
    "\n",
    "# def text_to_seq(wordIndex, tokens):\n",
    "#     seq = []\n",
    "#     for w in tokens:\n",
    "#         if w in wordIndex:\n",
    "#             seq.append(wordIndex[w])\n",
    "#         else:\n",
    "#             last_key = list(reversed(list(wordIndex)))[0]\n",
    "#             last_index = wordIndex[last_key]\n",
    "#             wordIndex[w] = last_index+1\n",
    "#             seq.append(wordIndex[w])\n",
    "#     return seq\n",
    "\n",
    "# u1_trainSequences, u2_trainSequences, u3_trainSequences = [text_to_seq(wordIndex, x) for x in u1_trainToken], [text_to_seq(wordIndex, x) for x in u2_trainToken], [text_to_seq(wordIndex, x) for x in u3_trainToken]\n",
    "# u1_valSequences, u2_valSequences, u3_valSequences = [text_to_seq(wordIndex, x) for x in u1_valToken], [text_to_seq(wordIndex, x) for x in u2_valToken], [text_to_seq(wordIndex, x) for x in u3_valToken]\n",
    "# u1_testSequences, u2_testSequences, u3_testSequences = [text_to_seq(wordIndex, x) for x in u1_testToken], [text_to_seq(wordIndex, x) for x in u2_testToken], [text_to_seq(wordIndex, x) for x in u3_testToken]\n",
    "                                                                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14532 unique tokens.\n",
      "Populating embedding matrix...\n",
      "Found 1193514 word vectors.\n",
      "[0.50440737 1.77731559 1.38006029 1.36944444]\n",
      "Starting k-fold cross validation...\n",
      "----------------------------------------\n",
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "# wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "print(\"Populating embedding matrix...\")\n",
    "out_of_vocab = []\n",
    "embeddingMatrix, _ = getEmbeddingMatrix(wordIndex, out_of_vocab)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(labels),\n",
    "                                             labels)\n",
    "print(class_weights)\n",
    "u1_data = pad_sequences(u1_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "u2_data = pad_sequences(u2_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "u3_data = pad_sequences(u3_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# labels = to_categorical(np.asarray(labels))\n",
    "# print(\"Shape of training data tensor: \", u1_data.shape)\n",
    "# print(\"Shape of label tensor: \", labels.shape)\n",
    "\n",
    "# Randomize data\n",
    "#np.random.shuffle(trainIndices)\n",
    "\n",
    "#u1_data = u1_data[trainIndices]\n",
    "#u2_data = u2_data[trainIndices]\n",
    "#u3_data = u3_data[trainIndices]\n",
    "\n",
    "#labels = labels[trainIndices]\n",
    "\n",
    "# Perform k-fold cross validation\n",
    "metrics = {\"accuracy\" : [],\n",
    "           \"microPrecision\" : [],\n",
    "           \"microRecall\" : [],\n",
    "           \"microF1\" : []}\n",
    "\n",
    "print(\"Starting k-fold cross validation...\")\n",
    "print('-'*40)\n",
    "print(\"Building model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922\n"
     ]
    }
   ],
   "source": [
    "print(len(out_of_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phenomennon', 'tensorflow', 'pursnal', 'interestingi', 'golmal', 'weaping', 'meesee', 'cilled', 'timd', 'cbillion', 'twelebrity', 'beera', 'padmavati', 'daznt', 'tiddi', 'propersion', 'e__e', 'aditeya', 'chatbot', 'sunnylenon', 'odrs', 'hoezaay', 'd=<', 'chakkha', '#emojisong', 'veryu', 'chatboard', 'doesnst', 'rddsw', 'ccool', 'understainding', 'heavinly', 'cmplt', 'assked', 'emogi', 'irritater', 'takae', 'denaerys', 'rnda', 'dhapar', 'dhamal', 'whaynyou', 'hanubot', 'heppin', 'getlst', 'lmaorofl', 'sychlops', 'prakrit', 'nicw', 'msgng', '#onekiss', 'd:', 'whatap', 'jesaus', 'complimentsnburns', 'kalsid', 'asach', 'pyrocms', 'qursion', 'half-girlfriend', 'anyonther', 'tyaima', 'beacus', 'useropenreflink', 'dellhi', 'thambs', '::', 'createar', 'lucj', 'wellfare', 'orajinal', 'decreats', 'togethere', 'meesengr', 'marrye', 'peoblem', 'whatodo', 'proosal', 'navratras', 'releaf', 'dindt', 'thjnk', 'vkski', 'libspil', 'loveb', '#bug', 'photodo', 'mbappe', ':/', 'deekila', 'enkoying', 'undestanding', 'rudw', 'naayy', 'dontvtalk', 'wgere', 'pilani', 'mersal', 'emchathunavuu', 'beatuhfull', 'asaid', 'sooperb', 'yourbwhatsapp', 'bakbus', 'umost', 'chinmay', 'peekabooh', 'relevantly', 'surpises', 'paib', 'malayali', 'peduthunavu', 'ashmit', 'reyale', 'inhiman', 'dirtu', '🏾', 'rachna', 'gotup', 'chingari', 'destrying', 'anhmote', 'notimes', 'vcook', 'lughing', 'ivvi', 'spagett', 'answrr', 'asips', 'istool', 'shaharukh', 'pohoto', 'interesing', 'sleepuly', 'englidh', 'alrays', 'cncrn', 'co-creators', 'navdha', 'massag', 'hereapple', 'engati', 'sendspacing', 'sycking', 'matol', 'cumed', 'bayys', 'hellium', 'isorry', 'imprees', 'fey-poehler', 'grapics', '#name', 'neans', 'gdxkgasgc', 'firstits', 'evining', 'aftaab', 'bangaram', 'uttra', 'cacter', 'madkolth', 'covinced', 'areally', 'notihing', 'landhy', 'dulcimer', 'pilige', 'eduku', 'uffd', '#offended', '🍾', 'aaraloverok', 'comdoms', 'tonmorrow', 'dicl', 'bikibi', 'fileg', 'craziiest', 'mhfy', 'sravani', 'chamical', 'overing', 'sooku', 'yashu', 'puthaa', 'barmounda', 'suscide', 'samrt', 'piscean', 'pampu', 'wronh', 'uttrakhand', 'kantikatopa', 'avadamledu', 'heppens', 'seelp', 'almightu', 'pakag', 'chepauk', 'aiyappa', 'ohter', 'whqt', 'feelong', 'onther', '⊄', 'friendchip', 'któw', 'christophee', 'imagemagick', 'pathsala', 'ooru', '#tab', 'sreya', 'hmm-zoning', '#iwantcakee', 'bholo', 'wadhiya', 'meabs', 'somach', 'bitter-sweetness', 'racict', 'nekde', 'baiber', 'toger', 'aosome', 'yes--i', 'suffrerring', 'photoes', 'stubed', 'yousi', 'ropley', 'mtlab', 'ignorese', 'frendzone', 'conneksun', 'andstay', 'sendme', 'brokened', 'ardam', 'sometimg', 'monostable', 'listicle', 'gramitacal', 'splitvilla', '.  .', 'lookinfinity', 'frindship', 'willife', 'sophesticated', 'terresa', 'lisbian', 'burfi', 'boribg', 'unavailability', 'businexs', 'vantatrfoul', 'aculty', 'humr', 'axact', 'dsturb', 'wouldkla', 'inhate', 'overemphasis', 'masabtank', 'togethera', 'ralation', 'nakhres', 'grandpaa', 'areeh', 'hyderbad', 'punctuating', 'oysterconnect', 'timwe', 'majourly', 'vistrat', 'tjhell', 'yeuggsggee', 'freecharge', 'dsrling', 'tomain', 'proffesionel', 'rodies', 'bharathidasan', 'epilespy', 'raceguram', 'hwere', 'ugborough', 'sejal', 'sonipt', 'hrll', 'kaneki', 'fouck', 'tkqa', 'masturbaated', 'everythingh', 'rediclous', '=p', 'njabi', '#ring', 'concatct', 'rattofied', 'holys', 'heroins', 'hshhshhdjh', 'katachi', 'sabji', 'sandip', '\\ue404', 'haird', 'srkfave', 'southy', 'speaked', 'kurozoka', 'tagaru', '>:/', 'ninety-ninety', 'upsate', '#chathero', 'brozoned', 'whoom', 'bbyee', 'progrer', 'comsidered', 'afcorse', 'inner-soul', 'abouf', 'intrsted', 'tejvanth', 'everybodu', 'flawer', 'rubingoogleapplelast', 'thint', 'lexgoww', 'zootropolis', 'ppast', 'relegion', 'wiollbe', 'qsts', 'immidiately', 'wisecrack', 'super-genius', 'uffo', 'mornon', 'stuid', 'nossence', 'naruto-esque', 'roties', 'worddive', 'askig', 'sirsa', 'haddent', 'awsmm', '‼', 'sampoornesh', 'beatd', 'northing', 'boilnes', 'nwadays', 'lolnelyguyabhi', 'aadu', 'pottypiece', 'jamuns', 'helatira', 'rakhenge', 'sajeev', 'dilbaro', 'esshh', 'histery', 'cmpare', 'chatbots', 'ichcha', 'offise', 'sreyashi', 'friends_q', 'kellah', 'numbar', 'tiping', 'disbalanced', 'kķk', 'mjnd', 'loivr', 'stronh', 'itbhere', 'ohhu', 'expoprt', 'mrowit', 'cccan', 'sick-ulars', 'welcommee', 'bestieq', 'osthada', 'clote', 'vhilf', 'fawret', 'solanum', 'intellegient', 'follish', 'marru', 'delayed-choice', '🖑', 'ownner', 'mculum', 'singele', 'headached', 'annpying', 'afterdat', 'menka', 'legd', 'voupen', 'yayayayyaayyayaya', 'tougj', 'doogy', 'smwt', 'idki', 'pickyourtab', 'canvideo', 'aloknath', 'korket', '️', 'proofe', 'barain', 'sahas', 'kurozuka', 'mondau', 'hextic', 'basicall', 'iterating', 'daurling', 'theets', 'zaeden', 'juilee', 'distik', 'firangi', 'ofit', 'adicter', 'typoed', 'execuse', 'chapathi', 'fnjd', 'arrugment', 'hveli', '₹', 'tanjaore', 'raftaar', 'afshi', 'anexious', 'bsccomputer', 'koninika', 'leavethree', 'pancajes', 'phosphade', 'poseley', 'shobana', 'havesome', 'whath', 'babuls', 'unutterably', 'sotyo', 'notlike', 'therilaye', 'gfrnd', 'tanmay', 'erutha', 'bannerlord', 'arjit', 'tammorw', 'arecieve', 'oufcourse', 'ignorence', 'whosover', 'nikanna', 'areceive', 'eelings', 'kranthi', '#change', 'tovmuch', 'commly', 'fhw', 'whonis', '#lonerforlife', 'kolambu', 'irritatingne', 'extar', 'uppala', 'abtbur', 'wouldesperately', 'khichdi', 'placce', 'vakil', 'atlteast', 'geniee', 'bsdk', 'yuujincho', 'opyrtc', 'eexcept', 'anyoney', 'likesne', 'feavaret', 'modaks', 'expexted', '(:', 'lattr', 'pohotes', 'loffing', 'dumbassee', 'x-min', 'kannad', 'rtfully', 'vadapav', 'chutia', 'movei', 'feesbook', 'ssaay', 'villege', 'readen', 'interogating', 'bhilai', 'wwwhhoo', 'concerntrate', 'haahm', 'cauging', 'liaes', 'playcreepypedia', 'horri-ble', 'kudhi', '#registeroyster', 'inali', 'jacquline', 'crady', 'divxplanet', 'abset', 'stept', 'huntee', 'anyonr', 'rajarajeshwari', 'hupoiuy', 'problemcom', 'semantically', 'dipressed', 'isapp', 'goodiredditus_in', 'bangram', 'feelingsd', 'jjftihc', 'hhahahhahahahah', 'asvice', 'febret', 'o-of', 'gentfinger', 'sucidal', 'notop', 'bhuvan', 'coquetry', 'fhec', 'faveourite', 'nambar', 'persuing', 'mapaas', 'chages', 'fumny', 'femous', 'bhamgarh', 'ameaning', 'reasorts', 'judawaa', 'iself', 'eishes', '<email>', 'wninner', 'equivelant', 'hugr', 'drishyam', 'understandig', 'autonomously', 'gobarr', 'nothat', 'gimal', 'jabardasth', 'realation', '#ouch', 'intentimes', 'junooniyat', 'hooma', 'orbitals', 'andwrite', 'ruih', 'stupif', 'corechat', 'krezy', 'hfdgj', '#registeroyester', 'adenoma', 'casess', 'cuddel', 'hyderabd', 'aniline', 'ncooah', 'happinesses', 'quikly', 'moust', 'qiestion', 'heeaadd', 'complicatedness', 'westworld', 'pachalam', 'devaiting', 'utlr', 'overwatch', 'mayandi', 'monologur', '#mspstatus', 'clemmson', 'overbold', 'duerstand', 'exaple', 'thinnava', 'mattaer', 'eare', 'barelly', 'jaaju', 'poinys', 'aindian', 'pgli', 'albace', 'makera', 'numner', 'eaen', 'dengau', '</a>', 'noonnono', 'sleaziest', 'gudni', 'dakna', 'ineedtogetalife', 'zootopia', 'rajsthan', 'prgent', 'rplies', 'ranbeer', 'pappad', '.  .  .  .  .  .', '#juniordoctorsam', 'goofle', 'gottisim', 'crearor', 'requiest', 'rajma', 'bhosdi', 'chicen', 'bueatyfull', 'nothe', 'gurjars', 'peogramer', 'socialgm', 'madayam', 'somedbody', 'areey', 'dushwoman', 'unrstnd', 'huyokbfr', 'reude', 'veer-zaara', 'timevis', 'betroot', 'arunava', 'suside', 'grilfernd', 'cbbyb', 'siviour', 'emabarassing', 'itӳ', 'languge', 'andhrapradesh', 'bhago', ';d', 'warssp', 'totapari', 'undersyand', 'clost', 'prapose', 'unmber', 'xombie', 'smoodles', 'picter', 'nothis', 'greeties', 'bakchod', 'context-less', 'kadhuly', 'deathzone', 'computerosed', 'rybgrbi', 'cardiopulmonary', 'idnot', 'exide', 'speaak', 'nptel', 'nigntmare', 'padayi', 'contnneu', 'winnipusakee', 'whatvis', 'unking', 'julh', 'dubaa', 'myby', 'whatsall', 'celob', 'blift', 'agien', 'bored-ly', 'fokkal', 'angru', 'tharof', 'eldg', 'brahmanandam', 'areepeating', 'beharan', ';/', 'questioons', 'uvaar', 'comouter', 'buggi', 'ex-cuse', 'moind', 'thinna', 'stupod', 'raatko', 'toungh', 'gubbi', 'depressede', 'hesrt', '🏼', '_u', 'yhour', 'selfishes', 'caupphechino', 'hainted', 'picec', 'yrss', 'welxome', 'stolef', 'sciencemba', 'undersrand', 'fighue', 'whére', 'panriya', 'gannam', 'redfort', 'searchs', 'rudeas', 'exspically', 'larts', 'dinr', 'oglf', 'funyy', 'conphyoojed', '\\u200d', 'peraon', 'rantir', 'awesomr', 'hahahahahhehehehehuhuhu', '.  .  .  .', 'brole', 'anamika', 'ftry', 'mohaabbatein', 'romontic', 'khalufa', 'justfine', 'niranjan', 'gardmara', 'catperson', 'jstu', 'irritatae', 'splyce', 'padmavat', 'lifep', 'nothey', 'up-to', 'hellr', 'merind', '🏿', 'abviously', 'gfvsg', 'tadybiar', 'werterners', 'iwillsetup', 'cheese-free', 'se-nse', 'udressed', 'wellsoon', 'connction', 'misuderstanding', 'sarakku', 'indimail', 'scansion', 'mybiyfriend', 'cursh', 'nerdyaddy', 'naangi', 'sreshta', 'coloer', '=(', 'anupamji', 'wishkey', 'studip', 'kiness', 'ex-hers', 'histetical', 'sensical', 'campliet', 'koregao', 'laier', 'charithra', 'fbest', 'oveyoutoo', 'tirupathi', 'hegels', 'niallsdip', 'hwll', 'antiope', 'placd', 'lelk', 'tummarow', 'allway', 'astroman', 'chechay', 'yeeuuh', 'cannote', 'skirllex', 'mkhi', 'hindhi', 'younifyou', '\\U0001f9d0', 'generalizes', 'topicz', 'rabdi', 'visiyed', 'mastrubating', 'stowage', 'stve', 'stuufs', 'rndi', 'phenol', 'asnwering', 'fuunya', 'partnet', 'foreheadwhen', 'ishappened', 'crein', 'fumb', 'fruittree', 'ghuess', 'wouldeveloper', 'whtass', 'hehehs', 'yuupss', 'hijre', 'slys', 'gunyt', 'mastee', 'llspiderman', 'zwian', 'huntai', 'wrriten', 'zingat', 'prejudistic', 'thnkq', 'awesonw', 'hagega', 'begar', 'miserya', 'spesifief', 'navodaya', 'sogs', 'nonveg', 'behevior', 'oookkeey', 'splooges', 'vastavaiya', 'loneeliest', 'stob', 'refrish', 'esquise', 'welu', 'palantiology', 'vaishnavi', 'digipom', 'spooke', 'amazning', 'bhavya', 'kkbye', 'ssly', 'flirthy', 'massege', 'willove', 'masterbuated', 'asgaard', 'alomr', 'opereter', 'sebd', 'updarling', 'newone', 'sarcasam', '\\U0001f92a', 'gadhi', 'noumber', 'alaone', 'wakda', 'lalso', 'benaz', 'wellbut', 'horrer', 'ctch', 'mystakes', 'bahubali', 'seand', 'hasbend', 'luok', 'beacous', 'notill', 'auestiond', 'ewho', 'madharchod', 'freiends', 'vlont', 'ugadhi', 'isupposed', 'oroblem', 'citizendium', 'dhakkan', 'rezzistance', 'movir', 'namper', 'dambu', 'omaxe', 'd=', 'usergetsstarted', 'infrastruture', 'tuitalk', 'actrss', 'collg', 'lololloo', 'hirika', 'sufficent', 'sweetei', 'bokachoda', 'gattu', 'shiuld', 'naaqs', ':=', 'naughy', 'fullbmeaning', 'fakme', 'cunfuse', 'phota', 'vizagite', 'ullee', 'sarvees', 'linkdn', 'rooely', 'emessage', 'prepaared', 'arebirth', 'watcging', 'actot', 'amcrying', '🏻', 'ideak', 'llthe', 'barmouda', 'normql', 'dipanshu', 'dustman', 'gachibowli', 'wailant', 'eidetic', 'natute', 'manishgi', 'kshimna', 'meeitate', 'pefok', 'sxual', 'ruuh', 'replyi', 'confuserd', 'teluu', 'picturw', 'pshcology', 'weinsteinchristopher', 'get-set-conquer', 'kothin', 'topıc', 'copl', '⁉', 'efoejgoegopseopfpoefopesfop', 'whrer', 'naashta', 'willady', 'rajstan', 'bhadrachalam', 'dalrs', 'cornform', 'chocolava', 'bentderesi', 'aapn', 'mahak', 'pakaoo', 'ptetty', 'hmmk', 'planings', 'okf', 'oforacle', 'bueaty', 'imittate', 'yesrerday', 'kopta', 'baords', 'judoon', 'enerjetic', 'ishqbazz', 'lstm', 'yilou', 'beffusy', 'tanro', 'ahdabaad', 'catosexual', 'dhjvfgj', 'vegitarians', 'tnzz', 'carch', 'laungages', 'lookings', 'pucy', 'grouw', 'heaten', 'bharatpur', 'tappichukodaniki', 'misundertandings', 'wworst', 'chaep', 'rubot', 'raght', 'handloom', 'vaccations', 'evercome', 'aapna', '#oysterstatus', 'sulare', 'srabani', 'serieas', 'yyyu', 'cigaratte', 'bollywoid', 'succha', 'aaking', 'badds', 'feeded', 'hhun', 'stolone', 'laugphin', 'isajid', 'attrocity', 'theriyadhu', 'donig', 'antaryaamini', 'bettry', 'kossed', 'libspill', 'carsh', 'strangr', 'laae', 'underweara', 'panipat', 'quicly', 'lallulal', 'sierri', '_not_', 'helri', 'bhosdike', 'bchfhmnvcffgghhjkhhggfvb', 'joaking', 'scuffletown', 'vkr', 'kikiyo', 'childreen', 'clock-radios', 'lotof', 'goodmrg', 'rehasal', 'hibberish', 'nimsy', 'rmtatooas', 'bayolover', 'badaam', 'okkati', 'naxt', 'hushaar', 'talav', 'speality', 'ridicilous', 'farrago', 'dibiler', 'krono', 'thnig', 'talket', 'yessd', 'dhiskiyaaon', 'chxk', 'scrab', 'amazingo', 'chavakkad', 'vegina', 'goodminton', 'mumbi', 'antadu', 'granz', 'adagaku', 'gkrina', 'indain', 'dhrr', 'boovs', 'painning', 'yucck', 'simplyee', 'kehch', 'diksha', 'langvej', '🏽', 'coupan', 'ranthambore', 'ctupid', 'padmavaat', 'panrel', 'nucleya', 'decript', 'antarchy', 'coefficients', 'un-learn', 'desaid', 'behabiour', 'udupy', 'engilsh', 'yayie', 'appatasiri', 'marag', 'techical', 'in-fact', 'nuumber', 'sanke', '<--', 'willike', 'aiyyoo', 'frnkness', 'haaww', 'wwrong', 'oviya', 'matchify', 'storu', 'answwr', 'maintenane', 'swadeshi', 'hairdone', 'flyingcow', 'boyfrienf', 'creepest', 'hichki', 'yurslf', 'hydrabad', 'ptoject', '#gametime', 'bhadvya', 'becauz', 'stomuch', 'frnklin', 'jeejus', 'romancee', 'hyoda', 'mastrbte', 'wouldisappointed', 'colddrink', 'riyl', 'excitefull', 'joiking', '/:', 'wwwhhaat', '\\U000fe339', 'padukon', 'ogopdegopjgopejogjoemgoesjfo', 'speciman', '.  .  .  .  .', 'humblee', 'coupen', 'fevee', 'hurter', 'internate', 'jokr', 'utell', 'dunb', 'cheran', 'begetarians', 'truth-conserving', 'chillaz', 'right-o', 'pribkems', 'uwere', 'lakmefw', 'fivar', 'namesakes', 'yaaraiyavthu', 'distrubed', 'a-rose', 'linetskypaul', 'roomee', 'maharhtra', 'tumbi', 'arejetted', 'mnutes', 'todya', 'okhh', 'pirn', 'tigar', 'notso', 'yeds', 'balaya', 'gudn', '</ss>', 'cllg', 'udaals', 'extc', 'nothem', 'swagath', 'chezt', 'webcamp', 'sujest', 'vachu', 'dhinchak', 'cerlac', 'idkwts', 'waatsup', 'intenlligent', 'cubieboard', 'sambhar', 'deskgod', 'elligible', 'giral', 'sngry', 'engring', 'areprogrammed', 'yyup', 'aboves', 'brkup', 'faimlly', 'humanitary', 'righth', 'amoviemm', 'ecactly', 'wprd', 'vedeo', 'refrs', 'agericalchar', 'whatsaup', 'iopp', 'nnoot', 'nanohana', 'egarliy', 'reality-she', 'enjoty', 'wouldo', 'futire', 'cheatet', 'fat-so', 'yweet', 'kislay', 'willaal-roomal', 'rommate', 'harst', 'open-eyed', 'hurey', 'kcubftv', 'mkre', 'kkrh', 'coupns', 'myelf', 'ysles', 'upsey', 'limitly', '):', 'baaghi', 'mivies', 'unabe', 'notil', 'diikshith', 'roboy', 'someoneteach', 'madkolr', 'hinglish', 'hereaftr', 'siatet', 'isomething', 'whereareyougoing', 'geetting', 'bthng', 'neyamma', 'chesthe', 'ayushi', 'universality', 'caviare', 'step-sisterzoned', 'goodbyw', '->', 'tyaanks', 'convrrsation', 'diagonals', 'answerer', 'willive', 'ametaphysics', 'darsheel', 'wasexpecting', 'awessome', 'avakin', 'deteals', 'aginc', 'birtch', 'derling', 'peyechen', 'hotstar', 'gsub', 'justdo', 'graduatn', 'frainds', 'nincompoops', 'tspsc', 'ranbiir', 'wakad', 'hfer', 'c-dac', 'wrick', 'chesukundam', 'rellyname', 'emailto', 'aboute', 'ahave', 'uwrong', 'atupid', 'yaaà', '.  .  .', 'nigusta', 'velieve', 'dyrtdd', 'insomiaa', 'vxxvgfeecbko', 'dirtynaughty', 'goshalls', 'refracts', 'robote', 'cgat', 'kufe', 'asshle', 'lyadh', 'characterless', 'palyed', 'grair', 'animenl', 'chattha', 'bdiya', 'arec', 'vahicle', 'rosein', 'averagae', 'pecular', 'doggosexual', 'chesthunnav', 'gameo', 'achivements', 'lebjoying', 'frirnd', 'shanananananana', 'chalp', 'kickley', 'igfisisfxfskxkxgksrxgkxtxfizgxgvhsch', 'cheppali', 'whatsappapp', 'tapsee', 'hapou', 'waitingh', 'reseat', 'pharchaun', 'vaago', 'asmita', 'nampr', 'tikda', 'studes', 'ofmine', 'sabudana', 'douing', 'wwll', 'autny', 'ambivert', 'hhhffh', 'consciouness', 'pluers', 'feevar', 'pagalni', 'fernds', 'banchod', 'formual', 'rogh', 'childing', 'surz', 'wlcum', '\\ue418', 'smlie', 'dinnaa', 'wakarimasen', 'refrral', 'perfedt', 'ratea', 'understandng', 'kerwaate', 'chuth', 'whtup', 'favoriate', 'manforce', 'datatype', 'prefarce', '\\u200b', 'repharse', 'kñew', 'exilante', 'weate', 'rightm', 'therichest', 'womderful', 'epabe', 'willol', 'ohhkk', 'tinderfessions', 'amaresh', 'dearymily', 'spuer', 'deewaad', 'angain', 'samreen', 'sutti', 'ckearly', 'ĺove', 'penchesukundhamane', 'truthes', 'dipression', 'loughing', 'ohko', 'krle', 'scienctist', 'isomeone', 'wdfhe', 'ypou', 'melbin', 'chàrlie', 'senceles', 'pridecof', 'haweli', 'dusht', 'seriya', 'ndnd', 'nabar', 'djxjv', ';p', 'gadhaa', 'unflinchingly', 'getng', 'cord-less', 'lavakusa', 'raply', 'xctly', 'bengalore', 'janakpuri', 'mastribute', 'charuta', 'umder', 'dhruva', 'nober', '(;', 'linux-sunxi', 'bittu', 'adangappa', 'realstik', 'chiiling', 'dhanyawad', 'liveng', '‑', 'bharadwaj', 'ashwairya', 'sexiguallowmeforsex', 'ahahaan', 'heenu', 'meeereeeppp', 'ranbeersingh', 'angoor', 'yfvx', 'thimes', 'genuienly', 'rudr', 'wahtsup', 'kavish', 'facuk', 'ishame', 'stupididty', 'relevently', 'syntaxes', 'mesgs', 'tppic', 'ryhope', 'sufring', 'relationnship', 'temperd', 'ytjph', '⏸', 'stomachace', 'uptill', 'charminar', '#playlot', 'çuz', 'bohemai', 'fuddu', 'ected', 'pluglass', 'bbhe', 'ireq', 'spleling', 'robut', 'moneyyuu', 'hsate', 'kalpita', 'taatto', 'girlfrieend', 'uhussup', 'secrare', 'vaijy', 'baijy', 'varigin', 'chteing', 'velapan', 'starnger', 'typhoo', '🏋', 'davel', 'agaij', 'aksed', 'gaayab', 'marrag', 'frod', 'zukerburg', 'hypoteticaly', 'microsofe', 'ishkwaji', 'donӵ', 'chooes', 'mitsuku', 'anuppu', 'worlsly', 'decsent', 'fovrat', 'kirollos', 'lonelt', 'finr', 'huhuhuhh', 'rampur', 'janeman', 'cchhe', '🤑', 'ilearnt', 'pyajamas', 'shallaa', 'rşch', 'inagree', 'handaome', 'rimajol', 'vergynit', 'nayis', 'cruah', 'beleiving', 'chtbt', 'brahmachari', 'avarage', 'bcouse', 'uoset', 'parfect', 'howyou', 'frootee', 'consious', 'deserver', 'rishabh', 'bhoma', 'smort', 'officent', 'displine', 'intligent', 'paramnet', 'suddnly', 'dunbo', 'besigeyou', 'chaust', 'zukergberg', 'coupans', 'hwru', 'zailet', 'pulihora', 'dumbk', 'contunue', 'whatӳ', 'siilence', 'sadke', 'peath', 'trawen', 'shmaby', 'inbucus', 'locx', 'delineated', 'ledha', 'sun-shined', 'wouldecent', 'khufia', 'englsih', '(=', 'bazigar', 'vaala', 'meenukkum', 'vilangu', 'rgerthgf', 'kandippa', 'de-select', 'adilbad', 'peddapalli', 'jardinains', 'you-sane', 'thinkpads', 'eboys', 'hydbadi', 'tostring', 'federlism', 'noturns', '_especially_', 'bed-room', 'be-droom', 'iitb', 'd-lete', 'computer-based', 'mrsing', 'cdac', 'eigenvector', 'checkedyo', 'gujrath', 'anopheles', 'salinity', 'ramnagar', 'moradabad', 'exjactly', 'exactlyhotel', 'patternsgood', 'codeproblem', 'empathies', '<-', 'aeronatutics', 'lol-give', 'tamizh', 'gurlfrnd', 'geetai', 'panjim', 'jayasurya', 'gujiyas', 'nownim', 'kitachi', 'ziphone', 'zariya', 'gdmg', 'deepawali', 'egotists', 'ishort', 'briting', 'moasing', 'boym', '🛰', 'abıt', 'tinava', 'sarukh', 'nudea', 'somrthing', 'piuture', 'inseminates', 'losy', 'verey', 'seriaesly', 'aartifical', 'usex', 'chandrabindu', 'fmail', 'tomoy', 'khjjfk', 'anubhab', 'wasepur', 'tattoing', 'prsnal', 'abilitity', 'vrry', 'mishane', 'facebouk', 'baffellow', 'analyzation', 'hindib', 'robort', 'rightniw', 'studay', 'somwone', 'whting', 'ainvy', 'launge', 'gudday', 'microsot', 'telematic', 'koribi', 'nooll', 'naatilinde', 'shariyaayilla', 'evenng', 'kdjhffjdkkd', 'ennakku', 'ttamilnatu', 'promgram', 'proseed', 'ozdgoisrgkfgj', 'kfjkjdfngrfdskfdfjlnfnfef', 'kushtagi', 'emotiom', 'thoae', 'abbai', 'dippresed', 'mmwaah', 'leqrn', 'sanuhoi', 'eshome', 'nothiny', 'nuded', 'loosst', 'bhadiya', 'tilltomorrow', 'thtat', 'zeeworld', 'nomoros', 'wipl', 'frndzone', 'cummimg', 'yiyou', 'judwaa', 'ubknow', 'jabardast', 'katharnac', 'animatrix', 'jujubi', 'wakenup', 'jamshedpur', 'medican', 'thowrn', 'senze', 'ywss', 'tellb', 'manupulating', 'dcac', 'semd', 'understtand', 'botking', 'vaoov', 'soported', '#hint', 'pohot', 'wrather', 'new-year', 'phoro', 'girfreind', 'agyo', 'chicolate', 'joim', 'deepikas', '⃣', 'rockzz', 'aayojan', 'ghazipur', '#avengers', 'swiggy', 'ruuch', 'voicemsg', 'mrunal', 'vakola', 'rolins', 'viji', 'srimathi', 'telme', 'dravidians', 'flep', 'fotographs', 'behenchood', 'underdiving', 'replyme', 'plessure', 'suoerb', 'softeware', 'sharaddha', 'berozgaari', 'lovear', 'wduwtta', '🛑', 'publicuty', 'zarvise', 'wrshp', 'nabr', 'vertual', 'gustion', 'pizzaass', 'dubstub', 'universiry', 'edsheerans', 'nsmer', 'alcheringa', 'scense', 'lovery', 'kewlwst', 'padmavathi', 'atrect', 'becauce', 'barthdey', 'atleas', 'swewty', 'smouch', 'milovie', 'ishaqbaaz', 'afgalgunj', 'higher-res', 'tihri', 'intreasted', ':d', 'withaut', 'ghoonghat', 'interperted', 'o__o', 'wouldigital', 'will-a', 'wouldumber', 'mondegar', 'isolid', 'hurman', 'isons', 'bhaskars', 'willocal', 'bbbyee', 'amayb', '\\ue409', 'almigthy', 'pokiri', 'lamakaan', '>:d', 'sovashvssjxj', 'tranwendy', 'exactllyy', 'egyup', 'iscary', 'sagarwcam', 'yaantey', '﹏', 'bachhe', 'tindering', 'vettai', 'cortanan', 'stinginess', 'intrast', 'programmin', 'zonbie', 'gujurati', 'dhyat', 'brozone', 'àapka', 'sunnyleone', 'lyier', 'panipiri', 'homour', 'silicon-based', 'raferal', 'emojisong', 'thanjavur', 'xwhat', 'devilliers', 'fidha', '#emojifortune', 'rameswaram', 'notjin', 'sircasm', 'introducted', 'ingish', 'listion', 'ookh', 'kudaellu', 'vegita', 'cleaverbot', 'spenkig', 'lovevu', 'eakada', 'unaru', 'secrt', 'seld', 'krbo', 'kanguage', 'spritiual', 'sometning', 'midnigt', 'gulthfriend', 'yyi', 'dunp', 'frenching', 'bioreactors', 'hounomkee', 'explean']\n"
     ]
    }
   ],
   "source": [
    "print(out_of_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724\n"
     ]
    }
   ],
   "source": [
    "with open('emoji/emoji_ranks.json', 'r', encoding=\"utf8\") as fn:\n",
    "    e_data = json.load(fn)\n",
    "\n",
    "pos_emoticons = e_data['pos']\n",
    "neg_emoticons = e_data['neg']\n",
    "neutral_emoticons = e_data['neu']\n",
    "print(len(pos_emoticons))\n",
    "\n",
    "# pos_emoticons.extend(['(:'])\n",
    "# neutral_emoticons.extend([''])\n",
    "# neg_emoticons.extend(['d=<'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"im fine 't  <NUMBER>   <NUMBER> th  <NUMBER> th  <NUMBER> st  <NUMBER> rd  <NUMBER> nd\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = \"im fine 't 12234234 9th 6th 1st 3rd 2nd\"\n",
    "re.sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", r\" <NUMBER> \", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "from enchant.checker import SpellChecker\n",
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sme at position 8\n",
      "fw at position 24\n",
      "speling at position 27\n",
      "\n",
      "This is sme text with a fw speling errors in it.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is sme text with a fw speling errors in it.\"\n",
    "\n",
    "chkr = SpellChecker(\"en_US\", text)\n",
    "for err in chkr:\n",
    "    print(err.word + \" at position \" + str(err.wordpos))\n",
    "    err.replace(err.word)\n",
    "\n",
    "t = chkr.get_text()\n",
    "print(\"\\n\" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySpellChecker():\n",
    "\n",
    "    def __init__(self, dict_name='en_US', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "\n",
    "        if suggestions:\n",
    "            for suggestion in suggestions:\n",
    "                if edit_distance(word, suggestion) <= self.max_dist:\n",
    "                    return suggestions[0]\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcommee at position 0\n",
      "\n",
      "welcommee\n"
     ]
    }
   ],
   "source": [
    "text = \"welcommee\"\n",
    "\n",
    "my_spell_checker = MySpellChecker(max_dist=1)\n",
    "chkr = SpellChecker(\"en_US\", text)\n",
    "for err in chkr:\n",
    "    print(err.word + \" at position \" + str(err.wordpos))\n",
    "    err.replace(my_spell_checker.replace(err.word))\n",
    "\n",
    "t = chkr.get_text()\n",
    "print(\"\\n\" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('tmp.csv', sep=',')\n",
    "df = pd.read_csv('Sentiment-Analysis-Dataset.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578624,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.SentimentText.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
